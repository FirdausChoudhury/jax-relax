{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "> Functions for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp legacy.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.legacy.import_essentials import *\n",
    "from relax.legacy.module import BaseTrainingModule, PredictiveTrainingModule\n",
    "from relax.legacy.logger import Logger\n",
    "from relax.legacy.utils import validate_configs, load_json\n",
    "from relax.legacy.ckpt_manager import CheckpointManager, load_checkpoint\n",
    "from relax.data_module import DataModule\n",
    "from urllib.request import urlretrieve\n",
    "from keras.src.trainers.epoch_iterator import EpochIterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingConfigs(BaseParser):\n",
    "    \"\"\"Configurator of `train_model`.\"\"\"\n",
    "    \n",
    "    n_epochs: int = Field(\n",
    "        description=\"Number of epochs.\"\n",
    "    )\n",
    "    batch_size: int = Field(\n",
    "        description=\"Batch size.\"\n",
    "    )\n",
    "    monitor_metrics: Optional[str] = Field(\n",
    "        None, description=\"Monitor metrics used to evaluate the training result after each epoch.\"\n",
    "    )\n",
    "    seed: int = Field(\n",
    "        42, description=\"Seed for generating random number.\"\n",
    "    )\n",
    "    log_dir: str = Field(\n",
    "        \"log\", description=\"The name for the directory that holds logged data during training.\"\n",
    "    )\n",
    "    logger_name: str = Field(\n",
    "        \"debug\", description=\"The name for the directory that holds logged data during training under log directory.\"\n",
    "    )\n",
    "    log_on_step: bool = Field(\n",
    "        False, description=\"Log the evaluate metrics at the current step.\"\n",
    "    )\n",
    "    max_n_checkpoints: int = Field(\n",
    "        3, description=\"Maximum number of checkpoints stored.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def PRNGSequence(self):\n",
    "        return hk.PRNGSequence(self.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_model_with_states(\n",
    "    training_module: BaseTrainingModule,\n",
    "    params: hk.Params,\n",
    "    opt_state: optax.OptState,\n",
    "    data_module: DataModule,\n",
    "    t_configs: Dict[str, Any] | TrainingConfigs,\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    \"\"\"Train models with `params` and `opt_state`.\"\"\"\n",
    "\n",
    "    t_configs = validate_configs(t_configs, TrainingConfigs)\n",
    "    keys = t_configs.PRNGSequence\n",
    "    # define logger\n",
    "    logger = Logger(\n",
    "        log_dir=t_configs.log_dir,\n",
    "        name=t_configs.logger_name,\n",
    "        on_step=t_configs.log_on_step,\n",
    "    )\n",
    "    logger.save_hyperparams(t_configs.dict())\n",
    "    if hasattr(training_module, \"hparams\") and training_module.hparams is not None:\n",
    "        logger.save_hyperparams(training_module.hparams)\n",
    "\n",
    "    training_module.init_logger(logger)\n",
    "    # define checkpoint manageer\n",
    "    if t_configs.monitor_metrics is None:\n",
    "        monitor_metrics = None\n",
    "    else:\n",
    "        monitor_metrics = f\"{t_configs.monitor_metrics}_epoch\"\n",
    "\n",
    "    ckpt_manager = CheckpointManager(\n",
    "        log_dir=Path(training_module.logger.log_dir) / \"checkpoints\",\n",
    "        monitor_metrics=monitor_metrics,\n",
    "        max_n_checkpoints=t_configs.max_n_checkpoints,\n",
    "    )\n",
    "    # dataloaders\n",
    "    # train_loader = jdl.DataLoader(jdl.ArrayDataset(*data_module['train']), backend='jax', batch_size=t_configs.batch_size, shuffle=True) \n",
    "    epoch_iterator = EpochIterator(*data_module['train'], batch_size=t_configs.batch_size, shuffle=True)\n",
    "    val_epoch_iterator = EpochIterator(*data_module['test'], batch_size=t_configs.batch_size, shuffle=False)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(params, opt_state, key, batch):\n",
    "        return training_module.training_step(params, opt_state, key, batch)\n",
    "    \n",
    "    # start training\n",
    "    for epoch in range(t_configs.n_epochs):\n",
    "        training_module.logger.on_epoch_started()\n",
    "        # for step, batch in epoch_iterator.enumerate_epoch('np'):\n",
    "        with tqdm(\n",
    "            epoch_iterator.enumerate_epoch('np'), \n",
    "            unit=\"batch\", \n",
    "            leave=epoch == t_configs.n_epochs - 1,\n",
    "            total=epoch_iterator.num_batches\n",
    "        ) as t_loader:\n",
    "            t_loader.set_description(f\"Epoch {epoch}\")\n",
    "            for step, batch in t_loader:\n",
    "                x, y = batch[0]\n",
    "                logs, (params, opt_state) = train_step(params, opt_state, next(keys), (x, y))\n",
    "                # TODO: tqdm becomes the bottleneck\n",
    "                t_loader.set_postfix(**logs)\n",
    "        \n",
    "        # validation\n",
    "        for step, batch in val_epoch_iterator.enumerate_epoch('np'):\n",
    "            x, y = batch[0]\n",
    "            logs = training_module.validation_step(params, next(keys), (x, y))\n",
    "            # logger.log(logs)\n",
    "        epoch_logs = training_module.logger.on_epoch_finished()\n",
    "        ckpt_manager.update_checkpoints(params, opt_state, epoch_logs, epoch)\n",
    "\n",
    "    training_module.logger.close()\n",
    "    return params, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_model(\n",
    "    training_module: BaseTrainingModule, # Training module\n",
    "    data_module: DataModule, # Data module\n",
    "    batch_size=128, # Batch size\n",
    "    epochs=1, # Number of epochs\n",
    "    **fit_kwargs # Positional arguments for `keras.Model.fit`\n",
    ") -> Tuple[hk.Params, optax.OptState]:\n",
    "    \"\"\"Train models.\"\"\"\n",
    "    t_configs = TrainingConfigs(\n",
    "        n_epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        **fit_kwargs\n",
    "    )\n",
    "    keys = t_configs.PRNGSequence \n",
    "    params, opt_state = training_module.init_net_opt(data_module, next(keys))\n",
    "    return train_model_with_states(\n",
    "        training_module=training_module,\n",
    "        params=params,\n",
    "        opt_state=opt_state,\n",
    "        data_module=data_module,\n",
    "        t_configs=t_configs,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "A siimple example to train a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.legacy.module import PredictiveTrainingModule, PredictiveModelConfigs\n",
    "from relax.data_module import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/legacy/ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 191/191 [00:01<00:00, 102.62batch/s, train/train_loss=0.06518286] \n"
     ]
    }
   ],
   "source": [
    "datamodule = load_data('adult')\n",
    "\n",
    "params, opt_state = train_model(\n",
    "    PredictiveTrainingModule({'sizes': [64, 32, 16], 'lr': 0.003}), \n",
    "    datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.ml_model import MLModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7148 - loss: 0.5726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<relax.ml_model.MLModule>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLModule()\n",
    "model.train(datamodule, batch_size=128, epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
