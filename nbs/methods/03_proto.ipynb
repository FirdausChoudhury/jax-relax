{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proto CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.base import TrainableMixedin, BaseConfig\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.utils import validate_configs, auto_reshaping, grad_update\n",
    "from relax.ml_model import AutoEncoder\n",
    "from relax.data_module import DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from relax.ml_model import MLModule\n",
    "from relax.data_module import load_data\n",
    "from relax.ml_model import load_ml_module\n",
    "import relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@ft.partial(jit, static_argnums=(2, 3, 9, 10, 12))\n",
    "def _proto_cf(\n",
    "    x: Array, \n",
    "    y_target: Array,\n",
    "    pred_fn: Callable[[Array], Array],\n",
    "    n_steps: int,\n",
    "    lr: float,\n",
    "    c: float, # hyperparameter for validity loss\n",
    "    beta: float, # cost = beta *l1_norm + l2_norm\n",
    "    gamma: float, # hyperparameter for loss_ae\n",
    "    theta: float, # hyperparameter for loss_proto\n",
    "    ae: keras.Model,\n",
    "    validity_fn: Callable,\n",
    "    sampled_data: Array,\n",
    "    apply_constraints_fn: Callable,\n",
    ") -> Array:\n",
    "    \n",
    "    @jit\n",
    "    def encode(x):\n",
    "        return ae.encoder(x)\n",
    "    \n",
    "    @jit\n",
    "    def cost_fn(cf, x):\n",
    "        # For some reasons, calling jnp.linalg.norm(cf - x) \n",
    "        # directly will lead to significant performance drop.\n",
    "        return beta * jnp.abs(cf - x).mean() + optax.l2_loss(cf, x).mean()\n",
    "    \n",
    "    @ft.partial(jit, static_argnums=(3))\n",
    "    def loss_fn(\n",
    "        cf: Array,\n",
    "        x: Array,\n",
    "        y_target: Array,\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "    ):\n",
    "        y_cf = pred_fn(cf)\n",
    "        loss_val = c * validity_fn(y_target, y_cf)\n",
    "        # loss_cost = beta * jnp.linalg.norm(cf - x, ord=1) + jnp.linalg.norm(cf - x, ord=2)\n",
    "        loss_cost = cost_fn(cf, x)\n",
    "        loss_ae = gamma * jnp.square(ae(cf) - cf).mean()\n",
    "        loss_proto = theta * jnp.square(\n",
    "            jnp.linalg.norm(encode(cf) - encode(sampled_data).sum(axis=0) / n_sampled_data, ord=2)\n",
    "        )\n",
    "        return (loss_val + loss_cost + loss_ae + loss_proto).mean()\n",
    "    \n",
    "    @loop_tqdm(n_steps)\n",
    "    def gen_cf_step(\n",
    "        i, cf_opt_state: Tuple[Array, optax.OptState] \n",
    "    ) -> Tuple[Array, optax.OptState]:\n",
    "        cf, opt_state = cf_opt_state\n",
    "        cf_grads = jax.grad(loss_fn)(cf, x, y_target, pred_fn)\n",
    "        cf, opt_state = grad_update(cf_grads, cf, opt_state, opt)\n",
    "        cf = apply_constraints_fn(x, cf, hard=False)\n",
    "        return cf, opt_state\n",
    "    \n",
    "    # Calculate the number of samples\n",
    "    # If the sampled data is all zeros, which means that this is not a valid sample.\n",
    "    # This is used to calculate the mean of encode(sampled_data)\n",
    "    n_sampled_data = jnp.where((sampled_data == 0).all(axis=1), 0, 1).sum()\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    opt = optax.adam(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    cf, opt_state = lax.fori_loop(0, n_steps, gen_cf_step, (cf, opt_state))\n",
    "    cf = apply_constraints_fn(x, cf, hard=True)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProtoCFConfig(BaseConfig):\n",
    "    \"\"\"Configurator of `ProtoCF`.\"\"\"\n",
    "    \n",
    "    n_steps: int = 100\n",
    "    lr: float = 0.01\n",
    "    c: float = Field(1, description=\"The weight for validity loss.\")\n",
    "    beta: float = Field(0.1, description=\"The weight for l1_norm in the cost function, where cost = beta * l1_norm + l2_norm.\")\n",
    "    gamma: float = Field(0.1, description=\"The weight for Autoencoder loss.\")\n",
    "    theta: float = Field(0.1, description=\"The weight for prototype loss.\")\n",
    "    n_samples: int = Field(128, description=\"Number of samples for prototype.\")\n",
    "    validity_fn: str = 'KLDivergence'\n",
    "    # AE configs\n",
    "    enc_sizes: List[int] = Field([64, 32, 16], description=\"List of hidden layers of Encoder.\")\n",
    "    dec_sizes: List[int] = Field([16, 32, 64], description=\"List of hidden layers of Decoder.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name of AutoEncoder.\")\n",
    "    ae_lr: float = Field(1e-3, description=\"Learning rate of AutoEncoder.\")\n",
    "    ae_loss: str = Field(\"mse\", description=\"Loss function name of AutoEncoder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProtoCF(ParametricCFModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict | ProtoCFConfig = None,\n",
    "        ae: keras.Model = None,\n",
    "        name: str = None,\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = ProtoCFConfig()\n",
    "        config = validate_configs(config, ProtoCFConfig)\n",
    "        self.ae = ae\n",
    "        name = \"ProtoCF\" if name is None else name\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: ProtoCFConfig, model: keras.Model, output_size: int):\n",
    "        if model is None:\n",
    "            model = AutoEncoder(\n",
    "                enc_sizes=config.enc_sizes,\n",
    "                dec_sizes=config.dec_sizes,\n",
    "                output_size=output_size,\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.get({\n",
    "                    'class_name': config.opt_name, \n",
    "                    'config': {'learning_rate': config.ae_lr}\n",
    "                }),\n",
    "                loss=config.ae_loss,\n",
    "            )\n",
    "        return model\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable = None,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Expected `data` to be `DataModule`, got type=`{type(data).__name__}` instead.\")\n",
    "        X_train, y_train = data['train'] \n",
    "        self.ae = self._init_model(self.config, self.ae, X_train.shape[1])\n",
    "        self.ae.fit(\n",
    "            X_train, X_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        # self.sampled_data = data.sample(self.config.n_samples)\n",
    "        sampled_xs, sampled_ys = data.sample(self.config.n_samples)\n",
    "        self.sampled_data = tuple(map(jax.device_put, (sampled_xs, sampled_ys)))\n",
    "        return self\n",
    "    \n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,  # `x` shape: (k,), where `k` is the number of features\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        y_target: Array = None,\n",
    "        **kwargs,\n",
    "    ) -> Array:\n",
    "        # TODO: Select based on the closest prototype.\n",
    "        if y_target is None:\n",
    "            y_target = 1 - pred_fn(x)\n",
    "        else:\n",
    "            y_target = jnp.array(y_target, copy=True).reshape(1, -1)\n",
    "\n",
    "        sampled_data = jnp.where(\n",
    "            y_target.argmax(axis=1) == self.sampled_data[1],\n",
    "            self.sampled_data[0],\n",
    "            jnp.zeros_like(self.sampled_data[0]),\n",
    "        )\n",
    "\n",
    "        return _proto_cf(\n",
    "            x=x,\n",
    "            y_target=y_target,\n",
    "            pred_fn=pred_fn,\n",
    "            n_steps=self.config.n_steps,\n",
    "            lr=self.config.lr,\n",
    "            c=self.config.c,\n",
    "            beta=self.config.beta,\n",
    "            gamma=self.config.gamma,\n",
    "            theta=self.config.theta,\n",
    "            ae=self.ae,\n",
    "            sampled_data=sampled_data,\n",
    "            validity_fn=keras.losses.get({'class_name': self.config.validity_fn, 'config': {'reduction': None}}),\n",
    "            apply_constraints_fn=self.apply_constraints,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n",
      "  warnings.warn(\"Passing `config` will have no effect.\")\n"
     ]
    }
   ],
   "source": [
    "dm = load_data('oulad')\n",
    "model = load_ml_module('oulad')\n",
    "xs_train, ys_train = dm['train']\n",
    "xs_test, ys_test = dm['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.1207   \n",
      "Epoch 2/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0418      \n",
      "Epoch 3/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0373      \n",
      "Epoch 4/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0341      \n",
      "Epoch 5/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0324    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ProtoCF>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcf = ProtoCF()\n",
    "pcf.set_apply_constraints_fn(dm.apply_constraints)\n",
    "pcf.train(dm, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3211fbda28cf46379c0943233c7f4136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  0.95471835\n"
     ]
    }
   ],
   "source": [
    "partial_gen = partial(pcf.generate_cf, pred_fn=model.pred_fn)\n",
    "cfs = vmap(partial_gen)(xs_test)\n",
    "\n",
    "print(\"Validity: \", keras.metrics.binary_accuracy(\n",
    "    (1 - model.pred_fn(xs_test)).round(),\n",
    "    model.pred_fn(cfs)\n",
    ").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
