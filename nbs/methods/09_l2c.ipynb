{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2C\n",
    "\n",
    "https://arxiv.org/abs/2209.13446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.l2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import *\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from keras_core.random import SeedGenerator\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "import relax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2C Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gumbel_softmax(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "):\n",
    "    \"\"\"The Gumbel softmax function.\"\"\"\n",
    "\n",
    "    gumbel_noise = jrand.gumbel(key, shape=logits.shape)\n",
    "    y = logits + gumbel_noise\n",
    "    return jax.nn.softmax(y / tau, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_categorical(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    "):\n",
    "    \"\"\"Sample from a categorical distribution.\"\"\"\n",
    "\n",
    "    def sample_cat(key, logits):\n",
    "        cat = jrand.categorical(key, logits=logits, axis=-1)\n",
    "        return jax.nn.one_hot(cat, logits.shape[-1])\n",
    "\n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_softmax(key, logits, tau=tau),\n",
    "        lambda _: sample_cat(key, logits),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = jnp.array([[2.0, 1.0, 0.1], [1.0, 2.0, 3.0]])\n",
    "key = jrand.PRNGKey(0)\n",
    "output = sample_categorical(key, logits, tau=0.5, training=True)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.allclose(output.sum(axis=-1), 1.0)\n",
    "# low temperature -> one-hot\n",
    "output = sample_categorical(key, logits, tau=0.01, training=True)\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")\n",
    "# high temperature -> uniform\n",
    "output = sample_categorical(key, logits, tau=100, training=True)\n",
    "assert jnp.max(output) - jnp.min(output) < 0.5\n",
    "\n",
    "output = sample_categorical(key, logits, tau=0.5, training=False)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_bernouli(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    prob: Array, # Logits for each class. Shape (batch_size, 1)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    ") -> Array:\n",
    "    \"\"\"\"Sample from a bernouli distribution.\"\"\"\n",
    "\n",
    "    def sample_ber(key, prob):\n",
    "        return jrand.bernoulli(key, p=prob).astype(prob.dtype)\n",
    "    \n",
    "    def gumbel_ber(key, prob, tau):\n",
    "        key_1, key_2 = jrand.split(key)\n",
    "        gumbel_1 = jrand.gumbel(key_1, shape=prob.shape)\n",
    "        gumbel_2 = jrand.gumbel(key_2, shape=prob.shape)\n",
    "        no_logits = (prob * jnp.exp(gumbel_1)) / tau\n",
    "        de_logits = no_logits + ((1. - prob) * jnp.exp(gumbel_2)) / tau\n",
    "        return no_logits / de_logits\n",
    "    \n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_ber(key, prob, tau),\n",
    "        lambda _: sample_ber(key, prob),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CModel(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_layers: list[int],\n",
    "        selector_layers: list[int],\n",
    "        feature_indices: list[tuple[int, int]] = None,\n",
    "        pred_fn: Callable = None,\n",
    "        alpha: float = 1e-4, # Sparsity regularization\n",
    "        tau: float = 0.7,\n",
    "        seed: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator_layers = generator_layers\n",
    "        self.selector_layers = selector_layers\n",
    "        self.feature_indices = feature_indices\n",
    "        self.pred_fn = pred_fn\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        seed = seed or get_config().global_seed\n",
    "        self.seed_generator = SeedGenerator(seed)\n",
    "\n",
    "    def set_features_info(self, feature_indices: list[tuple[int, int]]):\n",
    "        self.feature_indices = feature_indices\n",
    "        # TODO: check if the feature indices are valid\n",
    "\n",
    "    def set_pred_fn(self, pred_fn: Callable):\n",
    "        self.pred_fn = pred_fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_feats = len(self.feature_indices)\n",
    "        self.generator = MLP(\n",
    "            sizes=self.generator_layers,\n",
    "            output_size=input_shape[-1],\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"linear\",\n",
    "        )\n",
    "        self.selector = MLP(\n",
    "            sizes=self.selector_layers,\n",
    "            output_size=n_feats,\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def compute_l2c_loss(self, inputs, cfs, probs):\n",
    "        y_target = self.pred_fn(inputs).argmin(axis=-1)\n",
    "        y_pred = self.pred_fn(cfs)\n",
    "        validity_loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            y_target, y_pred\n",
    "        ).mean()\n",
    "        sparsity = jnp.linalg.norm(probs, ord=1) * self.alpha\n",
    "        return validity_loss, sparsity\n",
    "    \n",
    "    def perturb(self, inputs, cfs, probs, i, start, end):\n",
    "        return cfs[:, start:end] * probs[:, i : i + 1] + inputs[:, start:end] * (1 - probs[:, i : i + 1])\n",
    "    \n",
    "    def forward(self, inputs, training=False):\n",
    "        select_probs = self.selector(inputs, training=training)\n",
    "        probs = sample_bernouli(\n",
    "            self.seed_generator.next(), select_probs, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs_logits = self.generator(inputs, training=training)\n",
    "        cfs = sample_categorical(\n",
    "            self.seed_generator.next(), cfs_logits, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs = jnp.concatenate([\n",
    "                self.perturb(inputs, cfs, probs, i, start, end)\n",
    "                for i, (start, end) in enumerate(self.feature_indices)\n",
    "            ], axis=-1,\n",
    "        )\n",
    "        return cfs, probs\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        cfs, probs = self.forward(inputs, training=training)\n",
    "        # loss = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "        validity_loss, sparsity = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "        self.add_loss(validity_loss)\n",
    "        self.add_loss(sparsity)\n",
    "        return cfs   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut(\n",
    "    x: Array, # Input array\n",
    "    q: int, # Number of quantiles\n",
    "    axis: int = 0, # Axis to quantile\n",
    ") -> tuple[Array, Array]: # (digitized array, quantiles)\n",
    "    \"\"\"Quantile binning.\"\"\"\n",
    "    \n",
    "    # Handle edge cases: empty array or single element\n",
    "    if x.size <= 1:\n",
    "        return jnp.zeros_like(x), jnp.array([])\n",
    "    quantiles = jnp.quantile(x, jnp.linspace(0, 1, q + 1)[1:-1], axis=axis)\n",
    "    digitized = jnp.digitize(x, quantiles)\n",
    "    return digitized, quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "assert digitized.shape == (10,)\n",
    "assert quantiles.shape == (3,)\n",
    "assert jnp.allclose(\n",
    "    digitized, jnp.array([0,0,0,1,1,2,2,3,3,3])\n",
    ")\n",
    "\n",
    "quantiles_true = jnp.array([0, 2.25, 4.5, 6.75, 9])\n",
    "assert jnp.allclose(\n",
    "    quantiles, quantiles_true[1:-1]\n",
    ")\n",
    "x_empty = jnp.array([])\n",
    "q = 2\n",
    "digitized_empty, quantiles_empty = qcut(x_empty, q)\n",
    "assert digitized_empty.size == 0 and quantiles_empty.size == 0\n",
    "# Test with single element array\n",
    "x_single = jnp.array([1])\n",
    "digitized_single, quantiles_single = qcut(x_single, q)\n",
    "assert digitized_single.size == 1 and quantiles_single.size == 0\n",
    "\n",
    "# Test with large q value\n",
    "xs = jnp.array([1, 2, 3, 4, 5, 6])\n",
    "q_large = 10\n",
    "_, quantiles_large = qcut(xs, q_large)\n",
    "assert len(quantiles_large) == q_large - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut_inverse(\n",
    "    digitized: Array, # Digitized One-Hot Encoding Array\n",
    "    quantiles: Array, # Quantiles\n",
    ") -> Array:\n",
    "    \"\"\"Inverse of qcut.\"\"\"\n",
    "    \n",
    "    return digitized @ quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "ohe_digitized = jax.nn.one_hot(digitized, 4)\n",
    "quantiles_inv = qcut_inverse(ohe_digitized, jnp.arange(4))\n",
    "assert quantiles_inv.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cut_quantiles(\n",
    "    quantiles: Array, # Quantiles\n",
    "    xs: Array, # Input array\n",
    "):\n",
    "    quantiles = jnp.concatenate([\n",
    "        xs.min(axis=0, keepdims=True), \n",
    "        quantiles, \n",
    "        xs.max(axis=0, keepdims=True)\n",
    "    ])\n",
    "    quantiles = (quantiles[1:] + quantiles[:-1]) / 2\n",
    "    return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discretize_xs(\n",
    "    xs: Array, # Input array\n",
    "    is_categorical_and_indices: list[tuple[bool, tuple[int, int]]], # Features list\n",
    "    q: int = 4, # Number of quantiles\n",
    ") -> tuple[Array, list[Array], list[tuple[tuple[int, int], Array]]]: # (discretized array, indices_and_quantiles_and_mid)\n",
    "    \"\"\"Discretize continuous features.\"\"\"\n",
    "    \n",
    "    discretized_xs = []\n",
    "    indices_and_mid = []\n",
    "    quantiles_feats = []\n",
    "    discretized_start, discretized_end = 0, 0\n",
    "\n",
    "    for is_categorical, (start, end) in is_categorical_and_indices:\n",
    "        if is_categorical:\n",
    "            discretized, quantiles, mid = xs[:, start:end], None, None\n",
    "            discretized_end += end - start\n",
    "        else:\n",
    "            discretized, quantiles = qcut(xs[:, start:end].reshape(-1), q=q)\n",
    "            mid = cut_quantiles(quantiles, xs[:, start])\n",
    "            discretized = jax.nn.one_hot(discretized, q)\n",
    "            discretized_end += discretized.shape[-1]\n",
    "        \n",
    "        discretized_xs.append(discretized)\n",
    "        quantiles_feats.append(quantiles)\n",
    "        indices_and_mid.append(\n",
    "            ((discretized_start, discretized_end), mid)\n",
    "        )\n",
    "        \n",
    "        discretized_start = discretized_end\n",
    "    discretized_xs = jnp.concatenate(discretized_xs, axis=-1)\n",
    "    return discretized_xs, quantiles_feats, indices_and_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = relax.load_data(\"dummy\")\n",
    "xs, ys = dm['train']\n",
    "is_categorical_and_indices = [\n",
    "    (feat.is_categorical, indices) for feat, indices in zip(dm.features, dm.features.feature_indices)\n",
    "]\n",
    "discretized_xs, quantiles_feats, indices_and_mid = discretize_xs(xs, is_categorical_and_indices)\n",
    "assert discretized_xs.shape == (xs.shape[0], 4 * xs.shape[1])\n",
    "assert len(quantiles_feats) == len(is_categorical_and_indices)\n",
    "assert all(len(quantiles_feats[i]) == 3 for i in range(len(quantiles_feats)))\n",
    "assert len(indices_and_mid) == len(is_categorical_and_indices)\n",
    "assert all(len(indices_and_mid[i][1]) == 4 for i in range(len(indices_and_mid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Discretizer:\n",
    "    \"\"\"Discretize continuous features.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        is_cat_and_indices: list[tuple[bool, tuple[int, int]]], # Features list\n",
    "        q: int = 4 # Number of quantiles\n",
    "    ):\n",
    "        self.is_cat_and_indices = is_cat_and_indices\n",
    "        self.q = q\n",
    "\n",
    "    def fit(self, xs: Array):\n",
    "        _, self.quantiles, self.indices_and_mid_quantiles = discretize_xs(\n",
    "            xs, self.is_cat_and_indices, self.q\n",
    "        )\n",
    "\n",
    "    def transform(self, xs: Array):\n",
    "        digitized_xs = []\n",
    "        for quantiles, (_, (start, end)) in zip(self.quantiles, self.is_cat_and_indices):\n",
    "            if quantiles is None: \n",
    "                digitized = xs[:, start:end]\n",
    "            else:\n",
    "                digitized = jnp.digitize(xs[:, start], quantiles)\n",
    "                digitized = jax.nn.one_hot(digitized, self.q)\n",
    "            digitized_xs.append(digitized)\n",
    "        return jnp.concatenate(digitized_xs, axis=-1)\n",
    "\n",
    "    def fit_transform(self, xs: Array):\n",
    "        self.fit(xs)\n",
    "        return self.transform(xs)\n",
    "\n",
    "    def inverse_transform(self, xs: Array):\n",
    "        continutized_xs = []\n",
    "        for (start, end), mid_quantiles in self.indices_and_mid_quantiles:\n",
    "            if mid_quantiles is None:\n",
    "                cont_feat = xs[:, start:end]\n",
    "            else:\n",
    "                cont_feat = qcut_inverse(xs[:, start:end], mid_quantiles).reshape(-1, 1)\n",
    "            continutized_xs.append(cont_feat)\n",
    "        return jnp.concatenate(continutized_xs, axis=-1)\n",
    "    \n",
    "    def get_pred_fn(self, pred_fn: Callable[[Array], Array]):\n",
    "        def _pred_fn(xs: Array):\n",
    "            return pred_fn(self.inverse_transform(xs))\n",
    "        return _pred_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discretizer(is_categorical_and_indices)\n",
    "dis.fit(xs)\n",
    "digitized_xs_1 = dis.transform(xs)\n",
    "assert jnp.array_equal(discretized_xs, digitized_xs_1)\n",
    "inversed_xs = dis.inverse_transform(digitized_xs_1)\n",
    "assert xs.shape == inversed_xs.shape\n",
    "assert jnp.unique(inversed_xs).size == xs.shape[1] * 4\n",
    "\n",
    "ml_module = relax.load_ml_module(\"dummy\")\n",
    "pred_fn = dis.get_pred_fn(ml_module.pred_fn)\n",
    "y = pred_fn(digitized_xs_1)\n",
    "assert y.shape == (xs.shape[0], 2)\n",
    "\n",
    "def f(x, y):\n",
    "    y_pred = pred_fn(x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "grad = jax.grad(f)(digitized_xs_1, ys)\n",
    "assert grad.shape == digitized_xs_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2C Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CConfig(BaseConfig):\n",
    "    generator_layers: list[int] = Field(\n",
    "        [64, 64, 64], description=\"Generator MLP layers.\"\n",
    "    )\n",
    "    selector_layers: list[int] = Field(\n",
    "        [64], description=\"Selector MLP layers.\"\n",
    "    )\n",
    "    lr: float = Field(1e-3, description=\"Model learning rate.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name of training L2C.\")\n",
    "    alpha: float = Field(1e-4, description=\"Sparsity regularization.\")\n",
    "    tau: float = Field(0.7, description=\"Temperature for the Gumbel softmax.\")\n",
    "    q: int = Field(4, description=\"Number of quantiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2C(ParametricCFModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict | L2CConfig = None,\n",
    "        l2c_model: L2CModel = None,\n",
    "        name: str = \"l2c\",\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = L2CConfig()\n",
    "        config = validate_configs(config, L2CConfig)\n",
    "        name = name or \"l2c\"\n",
    "        self.l2c_model = l2c_model\n",
    "        super().__init__(config=config, name=name)\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Only support `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        \n",
    "        xs_train, ys_train = data['train']\n",
    "        self.discretizer = Discretizer(\n",
    "            [(feat.is_categorical, indices) for feat, indices in zip(data.features, data.features.feature_indices)],\n",
    "            q=self.config.q\n",
    "        )\n",
    "        discretized_xs_train = self.discretizer.fit_transform(xs_train)\n",
    "        pred_fn = self.discretizer.get_pred_fn(pred_fn)\n",
    "        features_indices = [indices for indices, _ in self.discretizer.indices_and_mid_quantiles]\n",
    "\n",
    "        self.l2c_model = L2CModel(\n",
    "            generator_layers=self.config.generator_layers,\n",
    "            selector_layers=self.config.selector_layers,\n",
    "            feature_indices=features_indices,\n",
    "            pred_fn=pred_fn,\n",
    "            alpha=self.config.alpha,\n",
    "            tau=self.config.tau,\n",
    "        )\n",
    "        self.l2c_model.compile(\n",
    "            optimizer=keras.optimizers.get({\n",
    "                'class_name': self.config.opt_name, \n",
    "                'config': {'learning_rate': self.config.lr}\n",
    "            }),\n",
    "            loss=None\n",
    "        )\n",
    "        self.l2c_model.fit(\n",
    "            discretized_xs_train, ys_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self, \n",
    "        x: Array, \n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        # TODO: Does not support passing apply_constraints        \n",
    "        discretized_x = self.discretizer.transform(x)\n",
    "        cfs, probs = self.l2c_model.forward(discretized_x, training=False)\n",
    "        return self.discretizer.inverse_transform(cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.1063     \n",
      "Epoch 2/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5560      \n",
      "Epoch 3/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3971     \n",
      "Epoch 4/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3766     \n",
      "Epoch 5/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3716     \n",
      "Epoch 6/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3701     \n",
      "Epoch 7/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3590     \n",
      "Epoch 8/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3423     \n",
      "Epoch 9/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3419     \n",
      "Epoch 10/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3363     \n"
     ]
    }
   ],
   "source": [
    "dm = relax.load_data('adult')\n",
    "ml_module = relax.load_ml_module('adult')\n",
    "l2c = L2C()\n",
    "exp = relax.generate_cf_explanations(\n",
    "    l2c, dm, ml_module.pred_fn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
