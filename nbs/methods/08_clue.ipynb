{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import CFModule, ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import auto_reshaping, validate_configs, get_config, grad_update\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from jax.scipy.stats.norm import logpdf as gaussian_logpdf\n",
    "from keras.random import SeedGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from relax.data_module import load_data\n",
    "from relax.ml_model import load_ml_module\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, sizes: List[int], dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert sizes[-1] % 2 == 0, f\"sizes[-1] must be even, but got {sizes[-1]}\"\n",
    "        self.encoder = keras.Sequential([\n",
    "            MLPBlock(size, dropout_rate=dropout) for size in sizes\n",
    "        ])\n",
    "    \n",
    "    def call(self, x: Array, training: bool):\n",
    "        params = self.encoder(x, training=training)\n",
    "        d = params.shape[-1] // 2\n",
    "        mu, sigma = params[:, :d], params[:, d:]\n",
    "        sigma = jax.nn.softplus(sigma)\n",
    "        sigma = jnp.clip(sigma, 1e-3)\n",
    "        return mu, sigma\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, sizes: List[int], output_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.decoder = MLP(\n",
    "            sizes, output_size=output_size, \n",
    "            dropout_rate=dropout, last_activation='sigmoid'\n",
    "        )\n",
    "    \n",
    "    def __call__(self, z: Array, training: bool):\n",
    "        mu_dec = self.decoder(z, training=training)\n",
    "        return mu_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = jrand.normal(jrand.PRNGKey(0), (100, 10))\n",
    "encoded_x = Encoder([100, 10])(inputs, training=True)\n",
    "assert encoded_x[0].shape == (100, 5)\n",
    "assert encoded_x[1].shape == (100, 5)\n",
    "\n",
    "decoded_x = Decoder([100, 10], 10)(inputs, training=True)\n",
    "assert decoded_x.shape == (100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jit\n",
    "def kl_divergence(p: Array, q: Array, eps: float = 2 ** -17) -> Array:\n",
    "    loss_pointwise = p * (jnp.log(p + eps) - jnp.log(q + eps))\n",
    "    return loss_pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAEGaussCat(keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_sizes: List[int] = [20, 16, 14, 12],\n",
    "        dec_sizes: List[int] = [12, 14, 16, 20],\n",
    "        dropout_rate: float = 0.1,    \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = enc_sizes\n",
    "        self.dec_sizes = dec_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed_generator = SeedGenerator(get_config().global_seed)\n",
    "        # default reconstruction loss to l2 loss\n",
    "        self.reconstruction_loss = lambda x, y: optax.l2_loss(x, y).mean(-1)\n",
    "\n",
    "    def set_reconstruction_loss(self, fn):\n",
    "        self.reconstruction_loss = fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.encoder = Encoder(self.enc_sizes, self.dropout_rate)\n",
    "        self.decoder = Decoder(self.dec_sizes, input_shape[-1], self.dropout_rate)\n",
    "    \n",
    "    def encode(self, x, training=True):\n",
    "        mu_z, var_z = self.encoder(x, training=training)\n",
    "        return mu_z, var_z\n",
    "    \n",
    "    def sample_latent(self, rng_key, mean, var):\n",
    "        key, _ = jax.random.split(rng_key)\n",
    "        std = jnp.exp(0.5 * var)\n",
    "        eps = jrand.normal(key, var.shape)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        reconstruct_x = self.decoder(z, training=training)\n",
    "        return reconstruct_x        \n",
    "    \n",
    "    def sample_step(self, rng_key, mean, var, training=True):\n",
    "        z = self.sample_latent(rng_key, mean, var)\n",
    "        mu_x = self.decode(z, training=training)\n",
    "        return mu_x\n",
    "    \n",
    "    def sample(self, x, mc_samples, training=True): # Shape: (mc_samples, batch_size, input_size)\n",
    "        mean, var = self.encode(x, training=training)\n",
    "        rng_key = self.seed_generator.next()\n",
    "        keys = jax.random.split(rng_key, mc_samples)\n",
    "        \n",
    "        partial_sample_step = partial(\n",
    "            self.sample_step, mean=mean, var=var, training=training\n",
    "        )\n",
    "        reconstruct_x = jax.vmap(partial_sample_step)(keys)\n",
    "        return (mean, var, reconstruct_x)\n",
    "        \n",
    "    def compute_vae_loss(self, inputs, mu_z, logvar_z, reconstruct_x):\n",
    "        kl_loss = -0.5 * (1 + logvar_z - jnp.power(mu_z, 2) - jnp.exp(logvar_z)).sum(-1)\n",
    "        \n",
    "        rec = self.reconstruction_loss(inputs, reconstruct_x.reshape(inputs.shape)).sum(-1)\n",
    "        batchwise_loss = (rec + kl_loss) / inputs.shape[0]\n",
    "        return batchwise_loss.mean()\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        mu_z, logvar_z, reconstruct_x = self.sample(inputs, mc_samples=1, training=training)\n",
    "        loss = self.compute_vae_loss(inputs, mu_z, logvar_z, reconstruct_x)\n",
    "        self.add_loss(loss)\n",
    "        return reconstruct_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n",
      "  warnings.warn(\"Passing `config` will have no effect.\")\n"
     ]
    }
   ],
   "source": [
    "vae_model = VAEGaussCat()\n",
    "vae_model.compile(optimizer=keras.optimizers.Adam(0.001), loss=None)\n",
    "dm = load_data('dummy')\n",
    "xs, _ = dm['train']\n",
    "history = vae_model.fit(\n",
    "    xs, xs,\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    verbose=0  # Set to 1 for training progress\n",
    ")\n",
    "assert history.history['loss'][0] > history.history['loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@ft.partial(jit, static_argnums=(3, 4, 6, 9, 12, 13))\n",
    "def _clue_generate(\n",
    "    x: Array,\n",
    "    rng_key: jrand.PRNGKey,\n",
    "    y_target: Array,\n",
    "    pred_fn: Callable,\n",
    "    max_steps: int,\n",
    "    step_size: float,\n",
    "    vae_module: VAEGaussCat,\n",
    "    uncertainty_weight: float,\n",
    "    aleatoric_weight: float,\n",
    "    prior_weight: float,\n",
    "    distance_weight: float,\n",
    "    validity_weight: float,\n",
    "    validity_fn: Callable,\n",
    "    apply_fn: Callable\n",
    ") -> Array:\n",
    "    \n",
    "    @jit\n",
    "    def sample_latent_from_x(x: Array, rng_key: jrand.PRNGKey):\n",
    "        key_1, key_2 = jrand.split(rng_key)\n",
    "        mean, var = vae_module.encode(x, training=False)\n",
    "        z = vae_module.sample_latent(key_2, mean, var)\n",
    "        return z\n",
    "    \n",
    "    @ft.partial(jit, static_argnums=(1))\n",
    "    def generate_from_z(z: Array, hard: bool):\n",
    "        cf = vae_module.decode(z, training=False)\n",
    "        cf = apply_fn(x, cf, hard=hard)\n",
    "        return cf\n",
    "\n",
    "    @jit\n",
    "    def uncertainty_from_z(z: Array):\n",
    "        cfs = generate_from_z(z, hard=False)\n",
    "        pred_cfs = pred_fn(cfs)\n",
    "        prob = pred_cfs[:, 1]\n",
    "        total_uncertainty = -(prob * jnp.log(prob + 1e-10)).sum(-1)\n",
    "        return total_uncertainty, cfs, pred_cfs\n",
    "    \n",
    "    @jit\n",
    "    def compute_loss(z: Array):\n",
    "        uncertainty, cfs, pred_cfs = uncertainty_from_z(z)\n",
    "        loglik = gaussian_logpdf(z).sum(-1)\n",
    "        dist = jnp.abs(cfs - x).mean()\n",
    "        validity = validity_fn(y_target, pred_cfs).mean()\n",
    "        loss = (\n",
    "            (uncertainty_weight + aleatoric_weight) * uncertainty \n",
    "            + prior_weight * loglik\n",
    "            + distance_weight * dist\n",
    "            + validity_weight * validity\n",
    "        )\n",
    "        return loss.mean()\n",
    "    \n",
    "    @loop_tqdm(max_steps)\n",
    "    def step(i, z_opt_state):\n",
    "        z, opt_state = z_opt_state\n",
    "        z_grad = jax.grad(compute_loss)(z)\n",
    "        z, opt_state = grad_update(z_grad, z, opt_state, opt)\n",
    "        return z, opt_state\n",
    "    \n",
    "    key_1, _ = jax.random.split(rng_key)\n",
    "    z = sample_latent_from_x(x, key_1)\n",
    "    opt = optax.adam(step_size)\n",
    "    opt_state = opt.init(z)\n",
    "\n",
    "    # Write a loop to optimize z using lax.fori_loop\n",
    "    z, opt_state = lax.fori_loop(0, max_steps, step, (z, opt_state))\n",
    "    cf = generate_from_z(z, hard=True)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a2f70c0fe34341abed691b0455183d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = xs[:1]\n",
    "pred_fn = load_ml_module('dummy').pred_fn\n",
    "cf = _clue_generate(\n",
    "    x,\n",
    "    jrand.PRNGKey(get_config().global_seed),\n",
    "    y_target=1 - pred_fn(x),\n",
    "    pred_fn=pred_fn,\n",
    "    max_steps=100,\n",
    "    step_size=0.1,\n",
    "    vae_module=vae_model,\n",
    "    uncertainty_weight=1.,\n",
    "    aleatoric_weight=1.,\n",
    "    prior_weight=1.,\n",
    "    distance_weight=1.,\n",
    "    validity_weight=1.,\n",
    "    validity_fn=keras.losses.get({'class_name': 'KLDivergence', 'config': {'reduction': None}}),\n",
    "    apply_fn=lambda x, cf, hard: cf\n",
    ")\n",
    "assert cf.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLUEConfig(BaseConfig):\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [20, 16, 14, 12], description=\"Sequence of Encoder layer sizes.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [12, 14, 16, 20], description=\"Sequence of Decoder layer sizes.\"\n",
    "    )\n",
    "    dropout_rate: float = Field(0.1, description=\"Dropout rate\")\n",
    "    encoded_size: int = Field(5, description=\"Encoded size\")\n",
    "    lr: float = Field(0.001, description=\"Learning rate\")\n",
    "    max_steps: int = Field(500, description=\"Max steps\")\n",
    "    step_size: float = Field(0.01, description=\"Step size\")\n",
    "    vae_n_epochs: int = Field(10, description=\"Number of epochs for VAE\")\n",
    "    vae_batch_size: int = Field(128, description=\"Batch size for VAE\")\n",
    "    seed: int = Field(0, description=\"Seed for random number generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_reconstruction_loss_fn(dm: DataModule):\n",
    "    def reconstruction_loss(xs, cfs):\n",
    "        losses = []\n",
    "        for feat, (start, end) in dm.features.features_and_indices:\n",
    "            if feat.is_categorical:\n",
    "                losses.append(\n",
    "                    optax.softmax_cross_entropy(cfs[:, start:end], xs[:, start:end]).reshape(-1, 1)\n",
    "                )\n",
    "            else:\n",
    "                losses.append(optax.l2_loss(cfs[:, start:end], xs[:, start:end]))\n",
    "        return jnp.concatenate(losses, axis=-1)\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n",
      "  warnings.warn(\"Passing `config` will have no effect.\")\n"
     ]
    }
   ],
   "source": [
    "dm = load_data('adult')\n",
    "reconstruction_loss = get_reconstruction_loss_fn(dm)\n",
    "xs, _ = dm['test']\n",
    "cfs = jrand.normal(jrand.PRNGKey(0), xs.shape)\n",
    "loss = reconstruction_loss(xs, cfs)\n",
    "assert loss.shape == (xs.shape[0], len(dm.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLUE(ParametricCFModule):\n",
    "\n",
    "    def __init__(self, config: Dict | CLUEConfig = None, vae=None, name: str = 'CLUE'):\n",
    "        if config is None:\n",
    "            config = CLUEConfig()\n",
    "        config = validate_configs(config, CLUEConfig)\n",
    "        self.vae = vae\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: CLUEConfig, model: VAEGaussCat):\n",
    "        if model is None:\n",
    "            model = VAEGaussCat(\n",
    "                enc_sizes=config.enc_sizes, dec_sizes=config.dec_sizes, \n",
    "                dropout_rate=config.dropout_rate\n",
    "            )\n",
    "            model.compile(optimizer=keras.optimizers.Adam(config.lr), loss=None)\n",
    "        return model\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, # data module\n",
    "        pred_fn: Callable = None,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Expected `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        X_train, y_train = data['train'] \n",
    "        self.vae = self._init_model(self.config, self.vae)\n",
    "        self.vae.fit(\n",
    "            X_train, X_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,\n",
    "        pred_fn: Callable,\n",
    "        y_target: Array = None,\n",
    "        rng_key: jrand.PRNGKey = None,\n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        # TODO: Currently assumes binary classification.\n",
    "        if y_target is None:\n",
    "            y_target = 1 - pred_fn(x)\n",
    "        else:\n",
    "            y_target = jnp.array(y_target, copy=True)\n",
    "        if rng_key is None:\n",
    "            raise ValueError(\"`rng_key` must be provided, but got `None`.\")\n",
    "        return _clue_generate(\n",
    "            x, \n",
    "            rng_key=rng_key, \n",
    "            y_target=y_target,\n",
    "            pred_fn=pred_fn,\n",
    "            max_steps=self.config.max_steps,\n",
    "            step_size=self.config.step_size,\n",
    "            vae_module=self.vae,\n",
    "            uncertainty_weight=.0,\n",
    "            aleatoric_weight=0.0,\n",
    "            prior_weight=0.0,\n",
    "            distance_weight=.1,\n",
    "            validity_weight=1.0,\n",
    "            validity_fn=keras.losses.get({'class_name': 'KLDivergence', 'config': {'reduction': None}}),\n",
    "            apply_fn=self.apply_constraints,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n",
      "  warnings.warn(\"Passing `config` will have no effect.\")\n"
     ]
    }
   ],
   "source": [
    "data = load_data('adult')\n",
    "pred_fn = load_ml_module('adult').pred_fn\n",
    "xs_train, ys_train = data['train']\n",
    "xs_test, ys_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1202    \n",
      "Epoch 2/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 0.0694     \n",
      "Epoch 3/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.0639    \n",
      "Epoch 4/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step - loss: 0.0621    \n",
      "Epoch 5/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.0613    \n"
     ]
    }
   ],
   "source": [
    "clue = CLUE()\n",
    "clue.train(data, batch_size=128, epochs=5)\n",
    "clue.set_apply_constraints_fn(data.apply_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a094767a015741d18e18934d1d4be165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = clue.generate_cf(xs_train[0], pred_fn, rng_key=jrand.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b67216467f641d7bc2eef652b9ffa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  0.16\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "n_tests = 100\n",
    "partial_gen = partial(clue.generate_cf, pred_fn=pred_fn)\n",
    "cfs = jax.vmap(partial_gen)(xs_test[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n",
    "\n",
    "assert cfs.shape == xs_test[:100].shape\n",
    "\n",
    "print(\"Validity: \", keras.metrics.binary_accuracy(\n",
    "    (1 - pred_fn(xs_test[:100])).round(),\n",
    "    pred_fn(cfs[:, :])\n",
    ").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
