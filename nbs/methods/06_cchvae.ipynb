{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCHVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.cchvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "from nbdev import show_doc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import CFModule, ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import auto_reshaping, grad_update, validate_configs, get_config\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from keras.random import SeedGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from relax.data_module import load_data\n",
    "from relax.ml_model import load_ml_module\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@keras.saving.register_keras_serializable()\n",
    "class CHVAE(keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers: list, \n",
    "        dropout_rate: float = 0., \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed_generator = SeedGenerator(get_config().global_seed)\n",
    "\n",
    "    def set_apply_constraints_fn(self, fn=None):\n",
    "        if fn is None:\n",
    "            fn = lambda x: x\n",
    "        self.apply_constraints = fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        encoder = keras.Sequential([\n",
    "            MLPBlock(size, dropout_rate=self.dropout_rate) for size in self.n_layers[:-1]\n",
    "        ])\n",
    "        decoder = keras.Sequential([\n",
    "            MLPBlock(size, dropout_rate=self.dropout_rate) for size in self.n_layers[::-1][1:]\n",
    "        ])\n",
    "        # Encoder\n",
    "        self.mu_enc = keras.Sequential([encoder, keras.layers.Dense(self.n_layers[-1])])\n",
    "        self.log_var_enc = keras.Sequential([encoder, keras.layers.Dense(self.n_layers[-1])])\n",
    "        # Decoder\n",
    "        self.mu_dec = keras.Sequential([\n",
    "            decoder, keras.layers.Dense(input_shape[-1]), \n",
    "            keras.layers.BatchNormalization(),\n",
    "        ])\n",
    "        self.log_var_dec = keras.Sequential([\n",
    "            decoder, keras.layers.Dense(input_shape[-1]), \n",
    "            keras.layers.BatchNormalization(),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x, training=None):\n",
    "        return self.mu_enc(x, training=training), self.log_var_enc(x, training=training)\n",
    "    \n",
    "    def decode(self, z, training=None):\n",
    "        return self.mu_dec(z, training=training), self.log_var_dec(z, training=training)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = keras.ops.exp(0.5 * log_var)\n",
    "        eps = keras.random.normal(std.shape, seed=self.seed_generator)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x, training=None):\n",
    "        mu_z, log_var_z = self.encode(x, training=training)\n",
    "        z = self.reparameterize(mu_z, log_var_z)\n",
    "        mu_x, log_var_x = self.decode(z, training=training)\n",
    "        return mu_z, log_var_z, z, mu_x, log_var_x\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        mu_z, log_var_z, z, mu_x, log_var_x = self.forward(x, training=training)\n",
    "        # compute loss\n",
    "        loss = self.compute_kl_loss(mu_z, log_var_z)\n",
    "        self.add_loss(loss)\n",
    "        reconstructed_x = mu_x\n",
    "        # reconstructed_x = self.apply_constraints(inputs, mu_x, hard=not training)\n",
    "        return reconstructed_x\n",
    "    \n",
    "    def regenerate(self, z):\n",
    "        mu, log_var = self.decode(z)\n",
    "        return mu\n",
    "    \n",
    "    def compute_kl_loss(self, mu, logvar):\n",
    "        kl_loss = -0.5 * keras.ops.sum(1 + logvar - mu**2 - keras.ops.exp(logvar))\n",
    "        return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _hyper_sphere_coordindates(\n",
    "    rng_key: jrand.PRNGKey, # Random number generator key\n",
    "    x: Array, # Input instance with only continuous features. Shape: (1, n_features)\n",
    "    n_samples: int,\n",
    "    high: float, # Upper bound\n",
    "    low: float, # Lower bound\n",
    "    p_norm: int = 2 # Norm\n",
    "):\n",
    "    key_1, key_2 = jrand.split(rng_key)\n",
    "    delta = jrand.normal(key_1, shape=(n_samples, x.shape[-1]))\n",
    "    dist = jrand.normal(key_2, shape=(n_samples,)) * (high - low) + low\n",
    "    norm_p = jnp.linalg.norm(delta, ord=p_norm, axis=1)\n",
    "    d_norm = jnp.divide(dist, norm_p).reshape(-1, 1)  # rescale/normalize factor\n",
    "    delta = jnp.multiply(delta, d_norm)\n",
    "    candidates = x + delta\n",
    "    return candidates\n",
    "     \n",
    "\n",
    "@ft.partial(jit, static_argnums=(3, 4, 5, 7))\n",
    "def _cchvae(\n",
    "    x: Array,\n",
    "    rng_key: jrand.PRNGKey,\n",
    "    y_target: Array,\n",
    "    pred_fn: Callable[[Array], Array],\n",
    "    max_steps: int,\n",
    "    n_search_samples: int,\n",
    "    step_size: float,\n",
    "    chvae: CHVAE,\n",
    "):\n",
    "    \"\"\"Counterfactual generation using CCHVAE.\"\"\"\n",
    "        \n",
    "    @loop_tqdm(max_steps)\n",
    "    def body_fn(i, state):\n",
    "        count, candidate_cf, rng = state\n",
    "        rng_key, subkey_1, subkey_2 = jrand.split(rng, num=3)\n",
    "        low, high = step_size * count, step_size * (count + 1)\n",
    "        # STEP 1 -- SAMPLE POINTS on hyper sphere around instance\n",
    "        latent_neighbors = _hyper_sphere_coordindates(\n",
    "            subkey_1, z_rep, n_search_samples, high=high, low=low, p_norm=1\n",
    "        )\n",
    "        x_ce = chvae.regenerate(latent_neighbors)\n",
    "        x_ce = chvae.apply_constraints(x, x_ce.reshape(1, -1), hard=True)\n",
    "        \n",
    "        # STEP 2 -- COMPUTE l1 norms\n",
    "        distances = jnp.abs(x_ce - x).sum(axis=1)\n",
    "\n",
    "        # STEP 3 -- SELECT POINT with MINIMUM l1 norm\n",
    "        y_candidates = pred_fn(x_ce).argmax(axis=1)\n",
    "        indices = jnp.where(y_candidates == y_target, 1, 0).astype(bool)\n",
    "        distances = jnp.where(indices, distances, jnp.inf)\n",
    "\n",
    "        best_candidate_cf = x_ce[jnp.argmin(distances)].reshape(1, -1)\n",
    "        \n",
    "        candidate_cf = lax.cond(\n",
    "            distances.min() < jnp.abs(x - candidate_cf).sum(axis=1).min(),\n",
    "            lambda _: best_candidate_cf,\n",
    "            lambda _: candidate_cf,\n",
    "            None\n",
    "        )\n",
    "\n",
    "        count += 1\n",
    "        return count, candidate_cf, rng_key\n",
    "    \n",
    "    y_target = y_target.reshape(1, -1).argmax(axis=1)\n",
    "    z, _ = chvae.encode(x)\n",
    "    # z_rep = jnp.repeat(z.reshape(1, -1), n_search_samples, axis=0)\n",
    "    z_rep = z.reshape(1, -1)\n",
    "    rng_key, _ = jrand.split(rng_key)\n",
    "    # candidate_cf = jnp.array(x, copy=True)\n",
    "    candidate_cf = jnp.ones_like(x) * jnp.inf\n",
    "    state = (0, candidate_cf, rng_key) # (count, candidate_cf, rng_key)\n",
    "    # count, candidate_cf, rng_key = jax.lax.while_loop(cond_fn, body_fn, state)\n",
    "    count, candidate_cf, rng_key = lax.fori_loop(0, max_steps, body_fn, state)\n",
    "    # if `inf` is found, return the original input\n",
    "    candidate_cf = jnp.where(jnp.isinf(candidate_cf), x, candidate_cf)\n",
    "    return candidate_cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CCHVAEConfig(BaseConfig):\n",
    "    vae_layers: List[int] = Field(\n",
    "        [20, 16, 14, 12], description=\"List of hidden layer sizes for VAE.\"\n",
    "    )\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name of VAE.\")\n",
    "    vae_lr: float = Field(0.001, description=\"Learning rate of VAE.\")\n",
    "    max_steps: int = Field(100, description=\"Max steps\")\n",
    "    n_search_samples: int = Field(100, description=\"Number of generated candidate counterfactuals.\")\n",
    "    step_size: float = Field(0.1, description=\"Step size\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CCHVAE(ParametricCFModule):\n",
    "    \n",
    "    def __init__(self, config: Dict | CCHVAEConfig = None, chvae=None, name: str = 'cchvae'):\n",
    "        if config is None:\n",
    "            config = CCHVAEConfig()\n",
    "        config = validate_configs(config, CCHVAEConfig)\n",
    "        name = \"CCHVAE\" if name is None else name\n",
    "        self.vae = chvae\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: CCHVAEConfig, model: keras.Model):\n",
    "        if model is None:\n",
    "            model = CHVAE(self.config.vae_layers)\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.get({\n",
    "                    'class_name': config.opt_name, \n",
    "                    'config': {'learning_rate': config.vae_lr}\n",
    "                }),\n",
    "                loss=keras.losses.MeanSquaredError()\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable = None,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Expected `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        X_train, y_train = data['train'] \n",
    "        self.vae = self._init_model(self.config, self.vae)\n",
    "        self.vae.set_apply_constraints_fn(data.apply_constraints)\n",
    "        self.vae.fit(\n",
    "            X_train, X_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,\n",
    "        pred_fn: Callable = None,\n",
    "        y_target: Array = None,\n",
    "        rng_key: jrand.PRNGKey = None,\n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        # TODO: Currently assumes binary classification.\n",
    "        if y_target is None:\n",
    "            y_target = 1 - pred_fn(x)\n",
    "        else:\n",
    "            y_target = jnp.array(y_target, copy=True)\n",
    "        if rng_key is None:\n",
    "            raise ValueError(\"`rng_key` must be provided, but got `None`.\")\n",
    "        \n",
    "        return _cchvae(\n",
    "            x,\n",
    "            rng_key=rng_key,\n",
    "            y_target=y_target,\n",
    "            pred_fn=pred_fn,\n",
    "            max_steps=self.config.max_steps,\n",
    "            n_search_samples=self.config.n_search_samples,\n",
    "            step_size=self.config.step_size,\n",
    "            chvae=self.vae,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/jax-relax/relax/data_module.py:234: UserWarning: Passing `config` will have no effect.\n",
      "  warnings.warn(\"Passing `config` will have no effect.\")\n"
     ]
    }
   ],
   "source": [
    "data = load_data('adult')\n",
    "pred_fn = load_ml_module('adult').pred_fn\n",
    "xs_train, ys_train = data['train']\n",
    "xs_test, ys_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 103.6776     \n",
      "Epoch 2/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 3.1196     \n",
      "Epoch 3/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - loss: 1.3849    \n",
      "Epoch 4/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.8786    \n",
      "Epoch 5/5\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 0.6225    \n"
     ]
    }
   ],
   "source": [
    "cchvae = CCHVAE()\n",
    "cchvae.train(data, epochs=5)\n",
    "cchvae.set_apply_constraints_fn(data.apply_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51d3e08aec4e32bd9e40620bca9c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = cchvae.generate_cf(xs_train[0], pred_fn, rng_key=jrand.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968424c2bf2d46bf868ea6d5fb7419c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  1.0\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "n_tests = 100\n",
    "partial_gen = partial(cchvae.generate_cf, pred_fn=pred_fn)\n",
    "cfs = jax.vmap(partial_gen)(xs_test[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n",
    "\n",
    "assert cfs.shape == xs_test[:100].shape\n",
    "\n",
    "print(\"Validity: \", keras.metrics.binary_accuracy(\n",
    "    (1 - pred_fn(xs_test[:100])).round(),\n",
    "    pred_fn(cfs[:, :])\n",
    ").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
