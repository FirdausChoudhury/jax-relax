{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAECF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.vaecf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from relax.utils import auto_reshaping, validate_configs, get_config\n",
    "from keras.random import SeedGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@jax.jit\n",
    "def hindge_embedding_loss(\n",
    "    inputs: Array, targets: Array, margin: float = 1.0\n",
    "):\n",
    "    \"\"\"Hinge embedding loss.\"\"\"\n",
    "    assert targets.shape == (1,)\n",
    "    loss = jnp.where(\n",
    "        targets == 1,\n",
    "        inputs,\n",
    "        jax.nn.relu(margin - inputs)\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "x = np.array([0.3, -0.1, 0.5, -0.5, .99, -.99])\n",
    "ys = np.array([-1])\n",
    "margin = 0.165\n",
    "assert jnp.isclose(\n",
    "    F.hinge_embedding_loss(torch.tensor(x), torch.tensor(ys), margin).cpu().numpy(),\n",
    "    hindge_embedding_loss(x, ys, margin).mean()\n",
    ")\n",
    "assert jnp.allclose(\n",
    "    F.hinge_embedding_loss(torch.tensor(x), torch.tensor(ys), margin, reduction='none').cpu().numpy(),\n",
    "    hindge_embedding_loss(x, ys, margin)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_latent(rng_key, mean, logvar):\n",
    "    eps = jax.random.normal(rng_key, mean.shape)\n",
    "    return mean + eps * jnp.sqrt(logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAE(keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        layers: list[int],\n",
    "        # pred_fn: Callable,\n",
    "        mc_samples: int = 50,\n",
    "        # compute_regularization_fn=None, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = layers\n",
    "        # self.pred_fn = pred_fn\n",
    "        self.mc_samples = mc_samples\n",
    "        self.rng_keys = SeedGenerator(get_config().global_seed)\n",
    "        # if compute_regularization_fn is None:\n",
    "        #     self.compute_regularization_fn = lambda *args, **kwargs: 0.\n",
    "        # elif callable(compute_regularization_fn):\n",
    "        #     self.compute_regularization_fn = compute_regularization_fn\n",
    "        # else:\n",
    "        #     raise ValueError(\"`compute_regularization_fn` must be callable or None, \",\n",
    "        #                      f\"but got {type(compute_regularization_fn)} instead.\")\n",
    "    \n",
    "    def set_pred_fn(self, pred_fn):\n",
    "        self.pred_fn = pred_fn\n",
    "\n",
    "    def set_compute_regularization_fn(self, compute_regularization_fn):\n",
    "        self.compute_regularization_fn = compute_regularization_fn\n",
    "\n",
    "    def _compile(self, x):\n",
    "        pred_out = self.pred_fn(x)\n",
    "        if pred_out.shape[-1] != 2: \n",
    "            raise ValueError(\"Only binary classification is supported.\")\n",
    "        \n",
    "        mu = self.mu_enc(x)\n",
    "        var = 0.5 + self.var_enc(x)\n",
    "        z = sample_latent(self.rng_keys.next(), mu, var)\n",
    "        z = jnp.concatenate([z, pred_out.argmax(-1, keepdims=True)], axis=-1)\n",
    "        mu_x = self.mu_dec(z)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        encoder = keras.Sequential([\n",
    "            MLPBlock(size, use_batch_norm=True, dropout_rate=0.) for size in self.n_layers[:-1]\n",
    "        ])\n",
    "        decoder = keras.Sequential([\n",
    "            MLPBlock(size, use_batch_norm=True, dropout_rate=0.) for size in self.n_layers[::-1][1:]\n",
    "        ])\n",
    "\n",
    "        self.mu_enc = keras.Sequential([encoder, keras.layers.Dense(self.n_layers[-1])])\n",
    "        self.var_enc = keras.Sequential([encoder, keras.layers.Dense(self.n_layers[-1], activation='sigmoid')])\n",
    "        self.mu_dec = keras.Sequential([\n",
    "            decoder, keras.layers.Dense(input_shape[-1]), \n",
    "        ])\n",
    "        self._compile(jnp.zeros(input_shape))\n",
    "\n",
    "    def encode(self, x, training=None):\n",
    "        mean = self.mu_enc(x, training=training)\n",
    "        var = 0.5 + self.var_enc(x, training=training)\n",
    "        return mean, var\n",
    "    \n",
    "    def decode(self, z, training=None):\n",
    "        return self.mu_dec(z, training=training)\n",
    "        \n",
    "    def sample(\n",
    "        self, \n",
    "        rng_key: jrand.PRNGKey, \n",
    "        inputs: Array, \n",
    "        mc_samples: int, \n",
    "        training=None\n",
    "    ):\n",
    "        @jit\n",
    "        def step(rng_key, em, ev, c):\n",
    "            # rng_key, _ = jrand.split(rng_key)\n",
    "            z = sample_latent(rng_key, em, ev)\n",
    "            z = jnp.concatenate([z, c], axis=-1)\n",
    "            mu_x = self.decode(z)\n",
    "            return mu_x\n",
    "\n",
    "        keys = jrand.split(rng_key, mc_samples)\n",
    "        x, c = inputs[:, :-1], inputs[:, -1:]\n",
    "        em, ev = self.encode(x, training=training)\n",
    "        step_fn = partial(step, em=em, ev=ev, c=c)\n",
    "        mu_x = jax.vmap(step_fn)(keys) # [mc_samples, n, d]\n",
    "        return em, ev, mu_x\n",
    "    \n",
    "    def compute_vae_loss(\n",
    "        self,\n",
    "        inputs: Array,\n",
    "        em, ev, cfs\n",
    "    ):\n",
    "        def cf_loss(cf: Array, x: Array, y: Array):\n",
    "            assert cf.shape == x.shape, f\"cf.shape ({cf.shape}) != x.shape ({x.shape}))\"\n",
    "            # proximity loss\n",
    "            recon_err = jnp.sum(jnp.abs(cf - x), axis=1).mean()\n",
    "            # Sum to 1 over the categorical indexes of a feature\n",
    "            cat_error = self.compute_regularization_fn(x, cf)\n",
    "            # validity loss\n",
    "            pred_prob = self.pred_fn(cf)\n",
    "            # This is same as the following:\n",
    "            # tempt_1, tempt_0 = pred_prob[y == 1], pred_prob[y == 0]\n",
    "            # validity_loss = hindge_embedding_loss(tempt_1 - (1. - tempt_1), -1, 0.165) + \\\n",
    "            #     hindge_embedding_loss(1. - 2 * tempt_0, -1, 0.165)\n",
    "            target = jnp.array([-1])\n",
    "            hindge_loss_1 = hindge_embedding_loss(\n",
    "                jax.nn.sigmoid(pred_prob[:, 1]) - jax.nn.sigmoid(pred_prob[:, 0]), target, 0.165)\n",
    "            hindge_loss_0 = hindge_embedding_loss(\n",
    "                jax.nn.sigmoid(pred_prob[:, 0]) - jax.nn.sigmoid(pred_prob[:, 1]), target, 0.165)\n",
    "            tempt_1 = jnp.where(y == 1, hindge_loss_1, 0).sum() / y.sum()\n",
    "            tempt_0 = jnp.where(y == 0, hindge_loss_0, 0).sum() / (y.shape[0] - y.sum())\n",
    "            validity_loss = tempt_1 + tempt_0\n",
    "            return recon_err + cat_error, - validity_loss\n",
    "        \n",
    "        xs, ys = inputs[:, :-1], inputs[:, -1]\n",
    "        kl = 0.5 * jnp.mean(em**2 + ev - jnp.log(ev) - 1, axis=1)\n",
    "        cf_loss_fn = partial(cf_loss, x=xs, y=ys)\n",
    "        cf_losses, validity_losses = jax.vmap(cf_loss_fn)(cfs)\n",
    "        return (cf_losses.mean() + kl).mean() + validity_losses.mean()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        rng_key = self.rng_keys.next()\n",
    "        ys = 1. - self.pred_fn(inputs).argmax(axis=1, keepdims=True)\n",
    "        inputs = jnp.concatenate([inputs, ys], axis=-1)\n",
    "        em, ev, cfs = self.sample(rng_key, inputs, self.mc_samples, training=training)\n",
    "        loss = self.compute_vae_loss(inputs, em, ev, cfs)\n",
    "        self.add_loss(loss)\n",
    "        return cfs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAECFConfig(BaseParser):\n",
    "    \"\"\"Configurator of `VAECFModule`.\"\"\"\n",
    "    layers: List[int] = Field(\n",
    "        [20, 16, 14, 12, 5],\n",
    "        description=\"Sequence of Encoder/Decoder layer sizes.\"\n",
    "    )\n",
    "    dropout_rate: float = Field(\n",
    "        0.1, description=\"Dropout rate.\"\n",
    "    )\n",
    "    opt_name: str = Field(\n",
    "        \"adam\", description=\"Optimizer name.\"  \n",
    "    )\n",
    "    lr: float = Field(\n",
    "        1e-3, description=\"Learning rate.\"\n",
    "    )\n",
    "    mc_samples: int = Field(\n",
    "        50, description=\"Number of samples for mu.\"\n",
    "    )\n",
    "    validity_reg: float = Field(\n",
    "        42.0, description=\"Regularization for validity.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VAECF(ParametricCFModule):\n",
    "    def __init__(self, config=None, vae=None, name: str = 'VAECF'):\n",
    "        if config is None:\n",
    "            config = VAECFConfig()\n",
    "        config = validate_configs(config, VAECFConfig)\n",
    "        self.vae = vae\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(\n",
    "        self, \n",
    "        config: VAECFConfig, \n",
    "        model: keras.Model, \n",
    "        # pred_fn: Callable,\n",
    "        # compute_regularization_fn: Callable\n",
    "    ):\n",
    "        if model is None:\n",
    "            model = VAE(\n",
    "                config.layers,\n",
    "                # pred_fn=pred_fn,\n",
    "                mc_samples=config.mc_samples,\n",
    "                # compute_regularization_fn=compute_regularization_fn\n",
    "            )\n",
    "            model.compile(\n",
    "                optimizer=keras.optimizers.get({\n",
    "                    'class_name': config.opt_name, \n",
    "                    'config': {'learning_rate': config.lr}\n",
    "                }),\n",
    "            )\n",
    "        return model\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable, \n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Expected `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        train_xs, train_ys = data['train']\n",
    "        self.vae = self._init_model(self.config, self.vae)\n",
    "        self.vae.set_pred_fn(pred_fn)\n",
    "        self.vae.set_compute_regularization_fn(data.compute_reg_loss)\n",
    "        self.vae.fit(\n",
    "            train_xs, train_ys, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,\n",
    "        pred_fn: Callable = None,\n",
    "        y_target: Array = None,\n",
    "        rng_key: jrand.PRNGKey = None,\n",
    "        **kwargs\n",
    "    ) -> Array:\n",
    "        # TODO: Currently assumes binary classification.\n",
    "        if y_target is None:\n",
    "            y_target = 1 - pred_fn(x).argmax(axis=1, keepdims=True)\n",
    "        else:\n",
    "            y_target = y_target.reshape(1, -1).argmax(axis=1, keepdims=True)\n",
    "        if rng_key is None:\n",
    "            raise ValueError(\"`rng_key` must be provided, but got `None`.\")\n",
    "                \n",
    "        @jit\n",
    "        def sample_step(rng_key, y_target):\n",
    "            inputs = jnp.concatenate([x, y_target], axis=-1)\n",
    "            _, _, cfs = self.vae.sample(rng_key, inputs, 1, training=False)\n",
    "            return cfs\n",
    "        \n",
    "        return sample_step(rng_key, y_target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data_module import load_data\n",
    "from relax.ml_model import load_ml_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = load_data('dummy')\n",
    "pred_fn = load_ml_module('dummy').pred_fn\n",
    "train_xs, train_ys = dm['train']\n",
    "test_xs, test_ys = dm['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaecf = VAECF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - loss: 10.8419\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 7.9748\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.1708\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 5.1577\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 4.3424\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.7982\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.3180\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.9390\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.6337\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.3621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VAECF>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaecf.train(dm, pred_fn, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = vaecf.generate_cf(test_xs[:1], pred_fn, rng_key=jrand.PRNGKey(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  0.55\n"
     ]
    }
   ],
   "source": [
    "n_tests = 100\n",
    "partial_gen = partial(vaecf.generate_cf, pred_fn=pred_fn)\n",
    "cfs = jax.vmap(partial_gen)(test_xs[:n_tests], rng_key=jrand.split(jrand.PRNGKey(0), n_tests))\n",
    "\n",
    "assert cfs.shape == test_xs[:100].shape\n",
    "\n",
    "print(\"Validity: \", keras.metrics.binary_accuracy(\n",
    "    (1 - pred_fn(test_xs[:100])).round(),\n",
    "    pred_fn(cfs[:, :])\n",
    ").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
