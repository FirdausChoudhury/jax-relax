{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from relax.import_essentials import *\n",
    "from relax.data_module import DataModule, DEFAULT_DATA_CONFIGS\n",
    "from relax.utils import validate_configs\n",
    "from relax.base import *\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    \"\"\"MLP block with leaky relu activation and dropout/batchnorm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        output_size: int, \n",
    "        dropout_rate: float = 0.3,\n",
    "        use_batch_norm: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        if use_batch_norm and dropout_rate > 0:\n",
    "            warnings.warn(\"Batch normalization and dropout are usually mutually exclusive.\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense = keras.layers.Dense(\n",
    "            self.output_size, activation='leaky_relu', \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.dropout = keras.layers.Dropout(self.dropout_rate)\n",
    "        if self.use_batch_norm:\n",
    "            self.batch_norm = keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm(x, training=training)\n",
    "        return x\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class MLP(keras.Model):\n",
    "    \"\"\"MLP model with multiple MLP blocks and a dense layer at the end.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        sizes: list, \n",
    "        output_size: int = 2,\n",
    "        dropout_rate: float = 0.3,\n",
    "        use_batch_norm: bool = False,\n",
    "        last_activation: str = 'softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.blocks = []\n",
    "        for size in sizes:\n",
    "            self.blocks.append(MLPBlock(size, dropout_rate, use_batch_norm))\n",
    "        self.dense = keras.layers.Dense(output_size, activation=last_activation)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        return self.dense(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'sizes': [block.output_size for block in self.blocks],\n",
    "            'output_size': self.dense.units,\n",
    "            'dropout_rate': self.blocks[0].dropout_rate,\n",
    "            'last_activation': self.dense.activation.__name__,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLModuleConfig(BaseConfig):\n",
    "    \"\"\"Configurator of `MLModule`.\"\"\"\n",
    "    \n",
    "    sizes: List[int] = Field([64, 32, 16], description=\"List of hidden layer sizes.\")\n",
    "    output_size: int = Field(2, description=\"The number of output classes.\")\n",
    "    dropout_rate: float = Field(0.3, description=\"Dropout rate.\")\n",
    "    lr: float = Field(1e-3, description=\"Learning rate.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name.\")\n",
    "    loss: str = Field(\"sparse_categorical_crossentropy\", description=\"Loss function name.\")\n",
    "    metrics: List[str] = Field([\"accuracy\"], description=\"List of metrics names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLModule(BaseModule, TrainableMixedin, PredFnMixedin):\n",
    "    def __init__(self, config: MLModuleConfig = None, *, model: keras.Model = None, name: str = None):\n",
    "        if config is None:\n",
    "            config = MLModuleConfig()\n",
    "        config = validate_configs(config, MLModuleConfig)\n",
    "        self.model = self._init_model(config, model)\n",
    "        self._is_trained = False\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: MLModuleConfig, model: keras.Model):\n",
    "        if model is None:\n",
    "            model = MLP(\n",
    "                sizes=config.sizes,\n",
    "                output_size=config.output_size,\n",
    "                dropout_rate=config.dropout_rate\n",
    "            )\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.get({\n",
    "                'class_name': config.opt_name, \n",
    "                'config': {'learning_rate': config.lr}\n",
    "            }),\n",
    "            loss=config.loss,\n",
    "            metrics=config.metrics\n",
    "        )\n",
    "        return model\n",
    "            \n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if isinstance(data, DataModule):\n",
    "            X_train, y_train = data['train']\n",
    "        else:\n",
    "            X_train, y_train = data\n",
    "        self.model.fit(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def is_trained(self) -> bool:\n",
    "        return self._is_trained\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True)\n",
    "        # self.model.save_weights(path / \"model.weights.h5\", overwrite=True)\n",
    "        self.model.save(path / \"model.keras\")\n",
    "        with open(path / \"config.json\", \"w\") as f:\n",
    "            json.dump(self.config.dict(), f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_path(cls, path):\n",
    "        path = Path(path)\n",
    "        config = MLModuleConfig(**json.load(open(path / \"config.json\")))\n",
    "        # model = keras.models.load_model(path / \"model.keras\")\n",
    "        model = keras.saving.load_model(path / \"model.keras\", compile=False)\n",
    "        module = cls(config, model=model)\n",
    "        module._is_trained = True\n",
    "        return module\n",
    "    \n",
    "    def pred_fn(self, x):\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model is not trained.\")\n",
    "        return self.model(x, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=5000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5163 - loss: 1.2818 \n",
      "Epoch 2/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7131 - loss: 0.6371\n",
      "Epoch 3/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7410 - loss: 0.5505\n",
      "Epoch 4/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7581 - loss: 0.5286\n",
      "Epoch 5/5\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7812 - loss: 0.4631\n"
     ]
    }
   ],
   "source": [
    "model = MLModule(\n",
    "    MLModuleConfig(sizes=[64, 32, 16],)\n",
    ")\n",
    "model.train((X_train, y_train), epochs=5)\n",
    "assert model.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = MLModule.load_from_path('tmp/model')\n",
    "assert model_1.is_trained\n",
    "assert np.allclose(model_1.pred_fn(X_test), model.pred_fn(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# remove tmp directory\n",
    "shutil.rmtree('tmp', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from relax.data_module import DataModule, DEFAULT_DATA_CONFIGS, load_data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# def train_ml_model_and_rf(data, config={}):\n",
    "#     dm = load_data(data)\n",
    "#     file_path = f\"assets/{data}/model/model.keras\"\n",
    "#     conf_path = f\"assets/{data}/model/config.json\"\n",
    "#     ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=file_path,\n",
    "#         monitor='val_accuracy',\n",
    "#         mode='max',\n",
    "#         save_best_only=True\n",
    "#     )\n",
    "#     train_xs, train_ys = dm['train']\n",
    "#     test_xs, test_ys = dm['test']\n",
    "#     model = MLModule(config).train(\n",
    "#         dm, validation_data=dm['test'], callbacks=[ckpt_cb]\n",
    "#     )\n",
    "#     model.config.save(conf_path)\n",
    "#     # Load the best model\n",
    "#     model = MLModule.load_from_path(f\"assets/{data}/model\")\n",
    "\n",
    "#     rf = RandomForestClassifier().fit(train_xs, train_ys.reshape(-1))\n",
    "#     rf_acc = accuracy_score(test_ys, rf.predict(test_xs))\n",
    "#     model_acc = accuracy_score(test_ys, model.pred_fn(test_xs).argmax(axis=1))\n",
    "#     return rf_acc, model_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# for data in DEFAULT_DATA_CONFIGS.keys():\n",
    "#     rf_acc, model_acc = train_ml_model_and_rf(data)\n",
    "#     if rf_acc > model_acc:\n",
    "#         models.append((data, rf_acc, model_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"dummy\"\n",
    "# dm = load_data(data)\n",
    "# file_path = f\"assets/{data}/model/model.keras\"\n",
    "# conf_path = f\"assets/{data}/model/config.json\"\n",
    "# ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=file_path,\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True\n",
    "# )\n",
    "# train_xs, train_ys = dm['train']\n",
    "# test_xs, test_ys = dm['test']\n",
    "# model = MLModule({\n",
    "#     'sizes': [128, 64, 32, 16],\n",
    "#     'dropout_rate': 0.3, 'lr': 0.001,\n",
    "#     'opt_name': 'adamw'\n",
    "# }).train(\n",
    "#     dm, validation_data=dm['test'], callbacks=[ckpt_cb], batch_size=64, epochs=10\n",
    "# )\n",
    "# model.config.save(conf_path)\n",
    "# # Load the best model\n",
    "# model = MLModule.load_from_path(f\"assets/{data}/model\")\n",
    "\n",
    "\n",
    "# rf = RandomForestClassifier().fit(train_xs, train_ys.reshape(-1))\n",
    "# rf_acc = accuracy_score(test_ys, rf.predict(test_xs))\n",
    "# model_acc = accuracy_score(test_ys, model.pred_fn(test_xs).argmax(axis=1))\n",
    "\n",
    "# rf_acc, model_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ML Module\n",
    "\n",
    "TODO: Need test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_ml_module(name: str, path: str = None):\n",
    "    if path is None:\n",
    "        path = Path('relax-assets') / name / 'model'\n",
    "    else:\n",
    "        path = Path(path)\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "    model_url = f\"https://huggingface.co/datasets/birkhoffg/ReLax-Assets/resolve/main/{name}/model/model.keras\"\n",
    "    config_url = f\"https://huggingface.co/datasets/birkhoffg/ReLax-Assets/resolve/main/{name}/model/config.json\"\n",
    "\n",
    "    if not (path / \"model.keras\").exists():\n",
    "        urlretrieve(model_url, filename=str(path / \"model.keras\"))\n",
    "    if not (path / \"config.json\").exists():\n",
    "        urlretrieve(config_url, filename=str(path / \"config.json\"))   \n",
    "    \n",
    "\n",
    "def load_ml_module(name: str) -> MLModule:\n",
    "    \"\"\"Load the ML module\"\"\"\n",
    "\n",
    "    if name not in DEFAULT_DATA_CONFIGS.keys():\n",
    "        raise ValueError(f'`data_name` must be one of {DEFAULT_DATA_CONFIGS.keys()}, '\n",
    "            f'but got data_name={name}.')\n",
    "\n",
    "    download_ml_module(name)\n",
    "    return MLModule.load_from_path(f\"relax-assets/{name}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in DEFAULT_DATA_CONFIGS.keys():\n",
    "    dm = load_data(name)\n",
    "    ml_model = load_ml_module(name)\n",
    "    X_train, y_train = dm['train']\n",
    "    X_test, y_test = dm['test']\n",
    "    model_acc = accuracy_score(y_test, ml_model.pred_fn(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AutoEncoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_sizes: List[int],\n",
    "        dec_sizes: List[int],\n",
    "        output_size: int,\n",
    "        dropout_rate: float = 0.2,\n",
    "        last_activation: str = \"sigmoid\",\n",
    "        name: str = \"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.encoder = keras.Sequential(\n",
    "            [MLPBlock(size, dropout_rate=dropout_rate) for size in enc_sizes]\n",
    "        )\n",
    "        self.decoder = keras.Sequential(\n",
    "            [MLPBlock(size, dropout_rate=dropout_rate) for size in dec_sizes]\n",
    "        )\n",
    "        self.output_layer = keras.layers.Dense(output_size, activation=last_activation)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        z = self.encoder(x, training=training)\n",
    "        x = self.decoder(z, training=training)\n",
    "        reconstructed = self.output_layer(x, training=training)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder([10, 5], [5, 10], output_size=10, last_activation=None)\n",
    "ae.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 161ms/step - loss: 0.3980\n",
      "Epoch 2/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3583\n",
      "Epoch 3/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3352\n",
      "Epoch 4/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3157\n",
      "Epoch 5/5\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_core.src.callbacks.history.History>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.fit(X_train, X_train, epochs=5, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
