{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "\n",
    "from relax.import_essentials import *\n",
    "from relax.base import *\n",
    "from relax.explain import *\n",
    "from keras.metrics import sparse_categorical_accuracy\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseEvalMetrics:\n",
    "    \"\"\"Base evaluation metrics class.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = None):\n",
    "        if name is None: \n",
    "            name = type(self).__name__\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        has_name = hasattr(self, 'name')\n",
    "        if not has_name:\n",
    "            raise ValidationError(\n",
    "                \"EvalMetrics must have a name. Add the following as the first line in your \"\n",
    "                f\"__init__ method:\\n\\nsuper({self.__name__}, self).__init__()\")\n",
    "        return self.name\n",
    "\n",
    "    def __call__(self, explanation: Explanation) -> Any:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PredictiveAccuracy(BaseEvalMetrics):\n",
    "    \"\"\"Compute the accuracy of the predict function.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"accuracy\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        xs, ys = explanation.xs, explanation.ys\n",
    "        pred_fn = explanation.pred_fn\n",
    "        pred_ys = pred_fn(xs)\n",
    "        accuracy = sparse_categorical_accuracy(ys, pred_ys)\n",
    "        return accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.98300004, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = PredictiveAccuracy()\n",
    "exp = fake_explanation(3)\n",
    "acc(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_single_validity(\n",
    "    xs: Array, # (n, d)\n",
    "    cfs: Array, # (n, d)\n",
    "    pred_fn: Callable[[Array], Array],\n",
    "):\n",
    "    y_xs = pred_fn(xs).argmax(axis=-1)\n",
    "    y_cfs = pred_fn(cfs).argmax(axis=-1)\n",
    "    validity = 1 - jnp.equal(y_xs, y_cfs).mean()\n",
    "    return validity\n",
    "\n",
    "def compute_validity(\n",
    "    xs: Array, # (n, d)\n",
    "    cfs: Array, # (n, d) or (n, b, d)\n",
    "    pred_fn: Callable[[Array], Array],\n",
    ") -> float:\n",
    "    cfs = einops.rearrange(cfs, 'n ... d -> n (...) d')\n",
    "    valdity_batch = jax.vmap(compute_single_validity, in_axes=(None, 1, None))(xs, cfs, pred_fn)\n",
    "    return valdity_batch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "xs = exp.xs\n",
    "cfs = exp.cfs\n",
    "pred_fn = exp.pred_fn\n",
    "\n",
    "assert jnp.isclose(\n",
    "    compute_validity(xs, cfs, pred_fn), 0.\n",
    ")\n",
    "assert jnp.isclose(\n",
    "    compute_validity(xs, cfs[:, 0, :], pred_fn), 0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Validity(BaseEvalMetrics):\n",
    "    \"\"\"Compute fraction of input instances on which CF explanation methods output valid CF examples.\n",
    "    Support binary case only.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"validity\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        xs, cfs, pred_fn = explanation.xs, explanation.cfs, explanation.pred_fn\n",
    "        return compute_validity(xs, cfs, pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = Validity()\n",
    "assert val(exp) == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_single_proximity(xs: Array, cfs: Array):\n",
    "    prox = jnp.linalg.norm(xs - cfs, ord=1, axis=1).mean()\n",
    "    return prox\n",
    "\n",
    "def compute_proximity(xs: Array, cfs: Array) -> float:\n",
    "    cfs = einops.rearrange(cfs, 'n ... d -> n (...) d')\n",
    "    prox_batch = jax.vmap(compute_single_proximity, in_axes=(None, 1))(xs, cfs)\n",
    "    return prox_batch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.isclose(\n",
    "    compute_proximity(xs, cfs, ), 0.\n",
    ")\n",
    "assert jnp.isclose(\n",
    "    compute_proximity(xs, cfs[:, 0, :], ), 0.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Proximity(BaseEvalMetrics):\n",
    "    \"\"\"Compute L1 norm distance between input datasets and CF examples divided by the number of features.\"\"\"\n",
    "    def __init__(self, name: str = \"proximity\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        xs, cfs = explanation.xs, explanation.cfs\n",
    "        return compute_proximity(xs, cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prox = Proximity()\n",
    "assert prox(exp) == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_single_sparsity(xs: Array, cfs: Array, feature_indices: List[Tuple[int, int]]):\n",
    "    def _feat_sparsity(xs, cfs, feat_indices):\n",
    "        start, end = feat_indices\n",
    "        xs = xs[:, start: end]\n",
    "        cfs = cfs[:, start: end]\n",
    "        return jnp.linalg.norm(xs - cfs, ord=0, axis=1).mean()\n",
    "    \n",
    "    return jnp.stack([_feat_sparsity(xs, cfs, feat_indices) for feat_indices in feature_indices]).mean()\n",
    "\n",
    "def compute_sparsity(xs: Array, cfs: Array, feature_indices: List[Tuple[int, int]]) -> float:\n",
    "    cfs = einops.rearrange(cfs, 'n ... d -> n (...) d')\n",
    "    sparsity_batch = jax.vmap(compute_single_sparsity, in_axes=(None, 1, None))(xs, cfs, feature_indices)\n",
    "    return sparsity_batch.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsity(BaseEvalMetrics):\n",
    "    \"\"\"Compute the number of feature changes between input datasets and CF examples.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str = \"sparsity\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        xs, cfs, feature_indices = explanation.xs, explanation.cfs, explanation.feature_indices\n",
    "        return compute_sparsity(xs, cfs, feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spar = Sparsity()\n",
    "assert spar(exp) == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@partial(jit, static_argnums=(2))\n",
    "def pairwise_distances(\n",
    "    x: Array, # [n, k]\n",
    "    y: Array, # [m, k]\n",
    "    metric: str = \"euclidean\" # Supports \"euclidean\" and \"cosine\"\n",
    ") -> Array: # [n, m]\n",
    "    def euclidean_distances(x: Array, y: Array) -> float:\n",
    "        XX = jnp.dot(x, x)\n",
    "        YY = jnp.dot(y, y)\n",
    "        XY = jnp.dot(x, y)\n",
    "        dist = jnp.clip(XX - 2 * XY + YY, a_min=0.)\n",
    "        return jnp.sqrt(dist)\n",
    "        # return jnp.linalg.norm(x - y, ord=2)\n",
    "    \n",
    "    def cosine_distances(x: Array, y: Array) -> float:\n",
    "        return 1.0 - jnp.dot(x, y) / (jnp.linalg.norm(x) * jnp.linalg.norm(y) + 1e-8)\n",
    "    \n",
    "    if metric == \"euclidean\":\n",
    "        dists_fn = vmap(vmap(euclidean_distances, in_axes=(None, 0)), in_axes=(0, None))\n",
    "    elif metric == \"cosine\":\n",
    "        dists_fn = vmap(vmap(cosine_distances, in_axes=(None, 0)), in_axes=(0, None))\n",
    "    else:\n",
    "        raise ValueError(f\"metric='{metric}' not supported\")\n",
    "    \n",
    "    return dists_fn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "@ft.partial(jax.jit, static_argnames=[\"k\", \"recall_target\"])\n",
    "def l2_ann(\n",
    "    qy, # Query vectors\n",
    "    db, # Database\n",
    "    k=10, # Number of nearest neighbors to return\n",
    "    recall_target=0.95 # Recall target for the approximation.\n",
    ") -> Tuple[Array, Array]: # Return (distance, neighbor_indices) tuples\n",
    "    dists = pairwise_distances(qy, db)\n",
    "    return jax.lax.approx_min_k(dists, k=k, recall_target=recall_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ManifoldDist(BaseEvalMetrics):\n",
    "    \"\"\"Compute the L1 distance to the n-nearest neighbor for all CF examples.\"\"\"\n",
    "    def __init__(self, n_neighbors: int = 1, name: str = \"manifold_dist\"):\n",
    "        super().__init__(name=name)\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        xs, cfs = explanation.xs, explanation.cfs\n",
    "        l2_ann_partial = ft.partial(l2_ann, k=self.n_neighbors)\n",
    "        dists, _ = vmap(l2_ann_partial, in_axes=(1, None))(cfs, xs)\n",
    "        return dists.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6.905339e-07, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man = ManifoldDist()\n",
    "man(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Runtime(BaseEvalMetrics):\n",
    "    \"\"\"Compute the runtime of the CF explanation method.\"\"\"\n",
    "    def __init__(self, name: str = \"runtime\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, explanation: Explanation) -> float:\n",
    "        return explanation.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = Runtime()\n",
    "run(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "METRICS_CALLABLE = [\n",
    "    PredictiveAccuracy('acc'),\n",
    "    PredictiveAccuracy('accuracy'),\n",
    "    Validity(),\n",
    "    Proximity(),\n",
    "    Runtime(),\n",
    "    ManifoldDist(),\n",
    "]\n",
    "\n",
    "METRICS = { m.name: m for m in METRICS_CALLABLE }\n",
    "\n",
    "DEFAULT_METRICS = [\"acc\", \"validity\", \"proximity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "for m in METRICS.keys(): assert isinstance(m, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_metric(metric: str | BaseEvalMetrics, cf_exp: Explanation):\n",
    "    if isinstance(metric, str):\n",
    "        if metric not in METRICS.keys():\n",
    "            raise ValueError(f\"'{metric}' is not supported. Must be one of {METRICS.keys()}\")\n",
    "        res = METRICS[metric](cf_exp)\n",
    "    elif callable(metric):\n",
    "        # f(cf_exp) not supported for now\n",
    "        if not isinstance(metric, BaseEvalMetrics):\n",
    "            raise ValueError(f\"metric needs to be a subclass of `BaseEvalMetrics`.\")\n",
    "        res = metric(cf_exp)\n",
    "    else:\n",
    "        raise ValueError(f\"{type(metric).__name__} is not supported as a metric.\")\n",
    "    \n",
    "    # Get scalar value\n",
    "    if isinstance(res, Array) and res.ravel().shape == (1,):\n",
    "        res = res.item()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "exp = fake_explanation()\n",
    "_acc_1 = _get_metric('acc', exp)\n",
    "test_fail(lambda: _get_metric('acc_1', exp), contains='is not supported')\n",
    "_acc_2 = _get_metric(PredictiveAccuracy(), exp)\n",
    "assert jnp.allclose(_acc_1, _acc_2)\n",
    "# functional callable not supported\n",
    "test_fail(lambda: _get_metric(Proximity, exp), contains='needs to be a subclass')\n",
    "test_fail(lambda: _get_metric(lambda: 1., exp), contains='needs to be a subclass') \n",
    "\n",
    "for m in METRICS_CALLABLE:\n",
    "    _res = _get_metric(m, exp)\n",
    "    assert isinstance(_res, (int, float))\n",
    "    assert not isinstance(_res, jnp.ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def evaluate_cfs(\n",
    "    cf_exp: Explanation, # CF Explanations\n",
    "    metrics: Iterable[Union[str, BaseEvalMetrics]] = None, # A list of Metrics. Can be `str` or a subclass of `BaseEvalMetrics`\n",
    "    return_dict: bool = True, # return a dictionary or not (default: True)\n",
    "    return_df: bool = False # return a pandas Dataframe or not (default: False)\n",
    "):\n",
    "    cf_name = cf_exp.cf_name\n",
    "    data_name = cf_exp.data_name\n",
    "    result_dict = { (data_name, cf_name): dict() }\n",
    "\n",
    "    if metrics is None:\n",
    "        metrics = DEFAULT_METRICS\n",
    "\n",
    "    for metric in metrics:\n",
    "        metric_name = str(metric)\n",
    "        result_dict[(data_name, cf_name)][metric_name] = _get_metric(metric, cf_exp)\n",
    "    result_df = pd.DataFrame.from_dict(result_dict, orient=\"index\")\n",
    "    \n",
    "    if return_dict and return_df:\n",
    "        return (result_dict, result_df)\n",
    "    elif return_dict or return_df:\n",
    "        return result_df if return_df else result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('dummy', 'dummy_method'): {'accuracy': 0.9830000400543213, 'validity': 0.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "exp = fake_explanation()\n",
    "evaluate_cfs(exp)\n",
    "evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"])\n",
    "d, df = evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"], return_df=True)\n",
    "assert isinstance(d, dict)\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "df = evaluate_cfs(exp, metrics=[\"acc\", \"validity\", \"proximity\", \"runtime\"], return_df=True, return_dict=False)\n",
    "assert isinstance(df, pd.DataFrame)\n",
    "\n",
    "evaluate_cfs(exp, metrics=[PredictiveAccuracy(), Validity()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def benchmark_cfs(\n",
    "    cf_results_list: Iterable[Explanation],\n",
    "    metrics: Optional[Iterable[str]] = None,\n",
    "):\n",
    "    dfs = [\n",
    "        evaluate_cfs(\n",
    "            cf_exp=cf_results, metrics=metrics, return_dict=False, return_df=True\n",
    "        )\n",
    "        for cf_results in cf_results_list\n",
    "    ]\n",
    "    return pd.concat(dfs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
